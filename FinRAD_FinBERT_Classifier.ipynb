{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinRAD_FinBERT_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPksQEplMNvPZa4rAWd3qF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sohomghosh/FinRAD_Financial_Readability_Assessment_Dataset/blob/main/FinRAD_FinBERT_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgjFhkGHJ4rB"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer, BertTokenizer, BertModel\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve,classification_report,confusion_matrix,precision_recall_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "get_ipython().magic('matplotlib inline')\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "path = \"/content/\"\n",
        "data = pd.read_csv(path + '/FinRAD_13K_terms_definitions_labels.csv')\n",
        "\n",
        "\n",
        "\n",
        "train_df, val_df = train_test_split(data, test_size=0.33, random_state=42)\n",
        "\n",
        "\n",
        "train_df['label_text'] = train_df['assigned_readability']\n",
        "val_df['label_text'] = val_df['assigned_readability']\n",
        "train_df.reset_index(inplace=True, drop = True)\n",
        "val_df.reset_index(inplace=True, drop = True)"
      ],
      "metadata": {
        "id": "jEdXlQ1t-t3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 64\n",
        "TRAIN_BATCH_SIZE = 256\n",
        "VALID_BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 2e-05\n",
        "model_name_details = \"ProsusAI/finbert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name_details)\n",
        "text_col_name = 'definitions'\n",
        "category_col = 'label_text'\n",
        "PATH = './finrad_fibert_classifier/'"
      ],
      "metadata": {
        "id": "-5UJY9rh-0-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Triage(Dataset):\n",
        "    \"\"\"\n",
        "    This is a subclass of torch packages Dataset class. It processes input to create ids, masks and targets required for model training. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len, text_col_name, category_col):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.text_col_name = text_col_name\n",
        "        self.category_col = category_col\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.data[self.text_col_name][index])\n",
        "        title = \" \".join(title.split())\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "        )\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(\n",
        "                self.data[self.category_col][index], dtype=torch.long\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "4PVA-5Vn_X-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Triage(train_df, tokenizer, MAX_LEN, text_col_name, category_col)\n",
        "validation_set = Triage(val_df, tokenizer, MAX_LEN, text_col_name, category_col)"
      ],
      "metadata": {
        "id": "DQ9Z6zBV_i3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loader parameters\n",
        "train_params = {\"batch_size\": TRAIN_BATCH_SIZE, \"shuffle\": True, \"num_workers\": 0}\n",
        "\n",
        "test_params = {\"batch_size\": VALID_BATCH_SIZE, \"shuffle\": False, \"num_workers\": 0}\n",
        "\n",
        "# creating dataloader for modelling\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(validation_set, **test_params)"
      ],
      "metadata": {
        "id": "Gh8s8oPh_qlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This is the modelling class which adds a classification layer on top of Roberta model. We finetune roberta while training for the label classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_class):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.l1 = BertModel.from_pretrained(model_name_details)\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, self.num_class)\n",
        "        self.history = dict()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "pXBWuPpH_2WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing and moving the model to the appropriate device\n",
        "model = BERTClass(len(my_dict))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "1i4F0L_P_8ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "QlEXSH3BAFIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accu(big_idx, targets):\n",
        "    \"\"\"\n",
        "    This function compares the predicted output with ground truth to give the count of the correct predictions.\n",
        "    \"\"\"\n",
        "    n_correct = (big_idx == targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "ERoLr7XWALkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    \"\"\"\n",
        "    Function to train the model. This function utilizes the model initialized using BERTClass. It trains the model and provides the accuracy on the training set.\n",
        "    \"\"\"\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _, data in enumerate(training_loader, 0):\n",
        "        ids = data[\"ids\"].to(device, dtype=torch.long)\n",
        "        mask = data[\"mask\"].to(device, dtype=torch.long)\n",
        "        targets = data[\"targets\"].to(device, dtype=torch.long)\n",
        "        outputs = model(ids, mask)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if _ % 250 == 0:\n",
        "            loss_step = tr_loss / nb_tr_steps\n",
        "            accu_step = (n_correct * 100) / nb_tr_examples\n",
        "            print(f\"Training Loss per 250 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 250 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}\")\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100) / nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_loss, epoch_accu"
      ],
      "metadata": {
        "id": "XHVGazzmAZ8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    \"\"\"\n",
        "    This function calculates the performance numbers on the validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    n_wrong = 0\n",
        "    total = 0\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
        "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
        "            targets = data[\"targets\"].to(device, dtype=torch.long)\n",
        "            outputs = model(ids, mask).squeeze()\n",
        "            loss = loss_function(outputs, targets)\n",
        "            tr_loss += loss.item()\n",
        "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "            n_correct += calcuate_accu(big_idx, targets)\n",
        "\n",
        "            nb_tr_steps += 1\n",
        "            nb_tr_examples += targets.size(0)\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    epoch_accu = (n_correct * 100) / nb_tr_examples\n",
        "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "    return epoch_loss, epoch_accu"
      ],
      "metadata": {
        "id": "ormHnbYiAbKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(PATH):\n",
        "    os.makedirs(PATH)\n",
        "\n",
        "# variable to store the model performance at the epoch level\n",
        "model.history[\"train_acc\"] = []\n",
        "model.history[\"val_acc\"] = []\n",
        "model.history[\"train_loss\"] = []\n",
        "model.history[\"val_loss\"] = []\n",
        "\n",
        "# model training\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Epoch number : \", epoch)\n",
        "    train_loss, train_accu = train(epoch)\n",
        "    val_loss, val_accu = valid(model, val_loader)\n",
        "    model.history[\"train_acc\"].append(train_accu)\n",
        "    model.history[\"train_loss\"].append(train_loss)\n",
        "    model.history[\"val_acc\"].append(val_accu)\n",
        "    model.history[\"val_loss\"].append(val_loss)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        },\n",
        "        PATH + \"/epoch_\" + str(epoch)+\"_\"+str(val_loss) + \".bin\",\n",
        "    )"
      ],
      "metadata": {
        "id": "AlV3P7S5AiY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring_data_prep(dataset = training_set):\n",
        "    out = []\n",
        "    target = []\n",
        "    mask = []\n",
        "    tf_idf_feature = []\n",
        "    for i in range(len(dataset)):\n",
        "   \n",
        "        rec = dataset[i]\n",
        "        out.append(rec['ids'].reshape(-1,MAX_LEN))\n",
        "        mask.append(rec['mask'].reshape(-1,MAX_LEN))\n",
        "        target.append(rec['targets'])\n",
        "\n",
        " \n",
        "\n",
        "        out_stack = torch.cat(out, dim = 0)\n",
        "        mask_stack = torch.cat(mask, dim =0 )\n",
        "        out_stack = out_stack.to(device, dtype = torch.long)\n",
        "        mask_stack = mask_stack.to(device, dtype = torch.long)\n",
        "\n",
        " \n",
        "\n",
        "        target_list = [i.item() for i in target]\n",
        "    return out_stack, mask_stack, target_list"
      ],
      "metadata": {
        "id": "7VFi8MnrBQ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_stack, mask_stack, target_list = scoring_data_prep(dataset = validation_set)\n",
        "n = 0\n",
        "batch_size = 500\n",
        "combined_output = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    while n < len(target_list):\n",
        "        output = model(out_stack[n:n+batch_size,:],mask_stack[n:n+batch_size,:])\n",
        "        n = n + batch_size\n",
        "        combined_output.append(output)\n",
        "        print(n)\n",
        "    combined_output = torch.cat(combined_output, dim = 0)\n",
        "    preds = torch.argsort(combined_output, axis = 1, descending = True)"
      ],
      "metadata": {
        "id": "bN8VdEzaBSOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = preds.to('cpu')\n",
        "actual_predictions = [i[0] for i in preds.tolist()]\n",
        "print(classification_report(target_list, actual_predictions))"
      ],
      "metadata": {
        "id": "m0jUR1SCBYyu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}